# main.py - XIAOZHI MCP SERVER v3.6 - TIERED MODEL SELECTION
import os
import asyncio
import json
import websockets
import requests
import logging
from flask import Flask, jsonify
import threading
import time
import sys
from dotenv import load_dotenv
import re
import random
import hashlib
from datetime import datetime, timedelta
import concurrent.futures

# ================= LOAD ENVIRONMENT VARIABLES =================
load_dotenv()

# Get configuration from environment variables
XIAOZHI_WS = os.environ.get("XIAOZHI_WS")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
CSE_ID = os.environ.get("CSE_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

# Validate critical configuration
if not XIAOZHI_WS:
    print("‚ùå ERROR: XIAOZHI_WS environment variable is not set!")
    print("   Go to Render.com dashboard ‚Üí Environment ‚Üí Add XIAOZHI_WS")
    sys.exit(1)

# ================= LOGGING SETUP =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ================= TIERED MODEL SELECTION =================
class SmartModelSelector:
    """Smart model selection based on YOUR exact tiers."""
    
    # YOUR EXACT TIERS
    TIERS = {
        "HARD": ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite"],
        "MEDIUM": ["gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.0-flash"],
        "SIMPLE": ["gemini-2.5-flash-lite"]
    }
    
    # Hard task keywords (use Pro)
    HARD_KEYWORDS = [
        'explain in detail', 'analyze', 'compare and contrast', 'evaluate',
        'write code for', 'debug', 'optimize', 'mathematical proof',
        'scientific analysis', 'research paper', 'thesis', 'essay',
        'comprehensive', 'detailed analysis', 'step by step guide',
        'programming', 'algorithm', 'complex', 'advanced', 'technical'
    ]
    
    # Medium task keywords (use Flash)
    MEDIUM_KEYWORDS = [
        'how to', 'what is', 'why does', 'when was', 'where is',
        'difference between', 'benefits of', 'advantages', 'disadvantages',
        'guide', 'tutorial', 'instructions', 'process', 'method',
        'explain', 'describe', 'summary', 'overview', 'basics of'
    ]
    
    # Simple task keywords (use Flash-Lite)
    SIMPLE_KEYWORDS = [
        'yes', 'no', 'ok', 'hello', 'hi', 'thanks', 'thank you',
        'weather', 'time', 'date', 'calculate', 'convert', 'simple',
        'fact', 'trivia', 'joke', 'quote', 'greeting', 'short answer',
        'quick', 'brief', 'define', 'meaning of', 'what time'
    ]
    
    @staticmethod
    def select_model(query):
        """Select model based on YOUR exact tiers."""
        query_lower = query.lower()
        words = len(query_lower.split())
        
        # Check for hard tasks first
        for keyword in SmartModelSelector.HARD_KEYWORDS:
            if keyword in query_lower:
                logger.info(f"üéØ HARD task detected: '{keyword}' ‚Üí Pro tier")
                return {
                    "tier": "HARD",
                    "models": SmartModelSelector.TIERS["HARD"],
                    "primary": "gemini-2.5-pro",
                    "tokens": 2000,
                    "timeout": 30
                }
        
        # Check for medium tasks
        for keyword in SmartModelSelector.MEDIUM_KEYWORDS:
            if keyword in query_lower:
                logger.info(f"üéØ MEDIUM task detected: '{keyword}' ‚Üí Flash tier")
                return {
                    "tier": "MEDIUM",
                    "models": SmartModelSelector.TIERS["MEDIUM"],
                    "primary": "gemini-2.5-flash",
                    "tokens": 1000,
                    "timeout": 20
                }
        
        # Check for simple tasks
        for keyword in SmartModelSelector.SIMPLE_KEYWORDS:
            if keyword in query_lower:
                logger.info(f"üéØ SIMPLE task detected: '{keyword}' ‚Üí Flash-Lite tier")
                return {
                    "tier": "SIMPLE",
                    "models": SmartModelSelector.TIERS["SIMPLE"],
                    "primary": "gemini-2.5-flash-lite",
                    "tokens": 500,
                    "timeout": 10
                }
        
        # Default based on query length
        if words > 20:
            logger.info(f"üéØ Long query ({words} words) ‚Üí HARD tier")
            return {
                "tier": "HARD",
                "models": SmartModelSelector.TIERS["HARD"],
                "primary": "gemini-2.5-pro",
                "tokens": 1500,
                "timeout": 25
            }
        elif words > 10:
            logger.info(f"üéØ Medium query ({words} words) ‚Üí MEDIUM tier")
            return {
                "tier": "MEDIUM",
                "models": SmartModelSelector.TIERS["MEDIUM"],
                "primary": "gemini-2.5-flash",
                "tokens": 800,
                "timeout": 15
            }
        else:
            logger.info(f"üéØ Short query ({words} words) ‚Üí SIMPLE tier")
            return {
                "tier": "SIMPLE",
                "models": SmartModelSelector.TIERS["SIMPLE"],
                "primary": "gemini-2.5-flash-lite",
                "tokens": 400,
                "timeout": 10
            }

# ================= GEMINI API CLIENT =================
gemini_cache = {}
CACHE_DURATION = 300

def call_gemini_api(query, model="gemini-2.5-flash", max_tokens=1000, timeout=20):
    """Call Gemini API with a specific model."""
    try:
        if not GEMINI_API_KEY:
            return "Gemini API key not configured."
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        data = {
            "contents": [{
                "parts": [{
                    "text": query
                }]
            }],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.7,
                "topP": 0.9,
                "topK": 40,
            }
        }
        
        params = {"key": GEMINI_API_KEY}
        
        response = requests.post(url, headers=headers, json=data, params=params, timeout=timeout)
        
        # Handle 404 - model not available
        if response.status_code == 404:
            logger.warning(f"‚ùå Model {model} not available (404)")
            return None, "MODEL_NOT_AVAILABLE"
        
        # Handle rate limits
        if response.status_code == 429:
            logger.warning(f"‚è∞ Model {model} rate limited (429)")
            return None, "RATE_LIMITED"
        
        response.raise_for_status()
        
        result = response.json()
        
        if "candidates" in result and len(result["candidates"]) > 0:
            candidate = result["candidates"][0]
            if "content" in candidate:
                parts = candidate["content"].get("parts", [])
                if parts and len(parts) > 0 and "text" in parts[0]:
                    answer = parts[0]["text"]
                    return answer, "SUCCESS"
        
        return None, "PARSE_ERROR"
        
    except requests.exceptions.Timeout:
        logger.warning(f"‚è∞ Model {model} timeout after {timeout}s")
        return None, "TIMEOUT"
    except Exception as e:
        logger.error(f"‚ùå Model {model} error: {e}")
        return None, "ERROR"

def ask_gemini_smart(query):
    """Smart Gemini query with YOUR tiered model selection."""
    try:
        if not query or not query.strip():
            return "Please provide a question."
        
        # Step 1: Select tier and model
        model_info = SmartModelSelector.select_model(query)
        tier = model_info["tier"]
        models_to_try = model_info["models"]
        primary_model = model_info["primary"]
        max_tokens = model_info["tokens"]
        timeout = model_info["timeout"]
        
        logger.info(f"ü§ñ Using {tier} tier: {primary_model} for '{query[:50]}...'")
        
        # Check cache first
        cache_key = hashlib.md5(f"{query}_{primary_model}".encode()).hexdigest()
        if cache_key in gemini_cache:
            cached_time, response = gemini_cache[cache_key]
            if datetime.now() - cached_time < timedelta(seconds=CACHE_DURATION):
                logger.info(f"‚ôªÔ∏è Cached response from {primary_model}")
                return f"[{tier} - Cached] {response}"
        
        # Step 2: Try models in tier order
        for model in models_to_try:
            logger.info(f"üîÑ Trying model: {model}")
            
            # Adjust tokens/timeout based on model
            if model == "gemini-2.5-pro":
                model_tokens = max_tokens
                model_timeout = timeout
            elif model == "gemini-2.5-flash":
                model_tokens = min(max_tokens, 1000)
                model_timeout = min(timeout, 20)
            else:  # flash-lite or 2.0-flash
                model_tokens = min(max_tokens, 500)
                model_timeout = min(timeout, 15)
            
            result, status = call_gemini_api(query, model, model_tokens, model_timeout)
            
            if status == "SUCCESS" and result:
                # Cache successful response
                gemini_cache[cache_key] = (datetime.now(), result)
                
                # Add tier/model info to response
                return f"[{tier} - {model}] {result}"
            
            elif status == "MODEL_NOT_AVAILABLE":
                logger.warning(f"‚ùå Model {model} not available, trying next in tier")
                continue
            
            elif status == "TIMEOUT":
                logger.warning(f"‚è∞ Model {model} timeout, trying next in tier")
                continue
            
            elif status == "RATE_LIMITED":
                # If rate limited, wait and try same model again
                logger.info(f"‚è≥ Rate limited on {model}, waiting 2s")
                time.sleep(2)
                result, status = call_gemni_api(query, model, model_tokens, model_timeout)
                if status == "SUCCESS" and result:
                    gemini_cache[cache_key] = (datetime.now(), result)
                    return f"[{tier} - {model}] {result}"
                continue
        
        # If all models in tier failed, fall back to basic
        logger.warning(f"‚ùå All models in {tier} tier failed, using basic fallback")
        return ask_gemini_basic(query)
        
    except Exception as e:
        logger.error(f"‚ùå Smart Gemini error: {e}")
        return f"AI error: {str(e)[:80]}"

def ask_gemini_basic(query):
    """Basic fallback if smart selection fails."""
    # Try the most reliable free models
    fallback_models = ["gemini-1.5-flash", "gemini-2.0-flash"]
    
    for model in fallback_models:
        result, status = call_gemini_api(query, model, 500, 10)
        if status == "SUCCESS" and result:
            return f"[Fallback - {model}] {result}"
    
    return "Gemini AI is currently unavailable. Please try again in a moment."

# ================= OTHER TOOLS (OPTIMIZED) =================
def google_search(query, max_results=10):
    """Google Search."""
    try:
        if not GOOGLE_API_KEY or not CSE_ID:
            return "Google Search not configured."
        
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": GOOGLE_API_KEY,
            "cx": CSE_ID,
            "q": query,
            "num": max_results,
            "safe": "active"
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "items" not in data or len(data["items"]) == 0:
            return "No results found."
        
        items = data["items"][:max_results]
        results = []
        
        for i, item in enumerate(items, 1):
            title = item.get('title', 'No title')
            link = item.get('link', 'No link')
            snippet = item.get('snippet', 'No description')
            
            result_text = f"{i}. **{title}**\n   üîó {link}\n   üìù {snippet}"
            results.append(result_text)
        
        return "\n\n".join(results)
        
    except Exception as e:
        logger.error(f"Google search error: {e}")
        return "Google search error."

def wikipedia_search(query, max_results=3):
    """Wikipedia Search."""
    try:
        url = "https://en.wikipedia.org/w/api.php"
        headers = {'User-Agent': 'XiaozhiBot/3.6'}
        
        search_params = {
            "action": "query",
            "format": "json",
            "list": "search",
            "srsearch": query,
            "srlimit": max_results,
            "utf8": 1
        }
        
        response = requests.get(url, params=search_params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "query" not in data or "search" not in data["query"]:
            return "No Wikipedia articles found."
        
        search_results = data["query"]["search"]
        if not search_results:
            return "No Wikipedia articles found."
        
        page_ids = [str(item["pageid"]) for item in search_results[:max_results]]
        
        extract_params = {
            "action": "query",
            "format": "json",
            "pageids": "|".join(page_ids),
            "prop": "extracts|info",
            "exintro": True,
            "explaintext": True,
            "inprop": "url",
            "exchars": 400
        }
        
        extract_response = requests.get(url, params=extract_params, headers=headers, timeout=10)
        extract_response.raise_for_status()
        extract_data = extract_response.json()
        
        pages = extract_data.get("query", {}).get("pages", {})
        
        results = []
        for i, page_id in enumerate(page_ids, 1):
            page = pages.get(page_id)
            if not page:
                continue
                
            title = page.get("title", "Unknown")
            extract = page.get("extract", "No summary available.")
            page_url = page.get("fullurl", f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}")
            
            if extract:
                extract = re.sub(r'\[\d+\]', '', extract)
                if len(extract) > 300:
                    extract = extract[:300] + "..."
            
            result_text = f"{i}. **{title}**\n   üåê {page_url}\n   üìñ {extract}"
            results.append(result_text)
        
        return "\n\n".join(results) if results else "No content retrieved."
        
    except Exception as e:
        logger.error(f"Wikipedia error: {e}")
        return "Wikipedia search error."

# ================= MCP PROTOCOL HANDLER =================
class MCPProtocolHandler:
    @staticmethod
    def handle_initialize(message_id):
        return {
            "jsonrpc": "2.0",
            "id": message_id,
            "result": {
                "protocolVersion": "2024-11-05",
                "capabilities": {"tools": {}},
                "serverInfo": {
                    "name": "smart-tier-gemini",
                    "version": "3.6.0"
                }
            }
        }
    
    @staticmethod
    def handle_tools_list(message_id):
        return {
            "jsonrpc": "2.0",
            "id": message_id,
            "result": {
                "tools": [
                    {
                        "name": "google_search",
                        "description": "Search Google",
                        "inputSchema": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string"}
                            },
                            "required": ["query"]
                        }
                    },
                    {
                        "name": "wikipedia_search",
                        "description": "Search Wikipedia",
                        "inputSchema": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string"}
                            },
                            "required": ["query"]
                        }
                    },
                    {
                        "name": "ask_ai",
                        "description": "Ask AI with SMART tiered model selection (Auto-chooses Pro/Flash/Flash-Lite)",
                        "inputSchema": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string"}
                            },
                            "required": ["query"]
                        }
                    }
                ]
            }
        }
    
    @staticmethod
    def handle_ping(message_id):
        return {"jsonrpc": "2.0", "id": message_id, "result": {}}
    
    @staticmethod
    def handle_tools_call(message_id, params):
        tool_name = params.get("name", "")
        query = params.get("arguments", {}).get("query", "").strip()
        
        if not query:
            return {
                "jsonrpc": "2.0",
                "id": message_id,
                "error": {"code": -32602, "message": "Missing query"}
            }
        
        try:
            if tool_name == "google_search":
                result = google_search(query)
            elif tool_name == "wikipedia_search":
                result = wikipedia_search(query)
            elif tool_name == "ask_ai":
                result = ask_gemini_smart(query)
            else:
                return {
                    "jsonrpc": "2.0",
                    "id": message_id,
                    "error": {"code": -32601, "message": f"Unknown tool: {tool_name}"}
                }
            
            return {
                "jsonrpc": "2.0",
                "id": message_id,
                "result": {"content": [{"type": "text", "text": result}]}
            }
            
        except Exception as e:
            logger.error(f"Tool error: {e}")
            return {
                "jsonrpc": "2.0",
                "id": message_id,
                "error": {"code": -32000, "message": f"Error: {str(e)[:80]}"}
            }

# ================= WEB SERVER =================
app = Flask(__name__)
server_start_time = time.time()

@app.route('/')
def index():
    uptime = int(time.time() - server_start_time)
    hours, remainder = divmod(uptime, 3600)
    minutes, seconds = divmod(remainder, 60)
    
    return f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Xiaozhi MCP v3.6</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body {{ font-family: -apple-system, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
            .header {{ background: linear-gradient(135deg, #4285F4 0%, #34A853 100%); 
                      color: white; padding: 2rem; border-radius: 10px; margin-bottom: 2rem; }}
            .tier {{ padding: 1rem; margin: 1rem 0; border-radius: 8px; }}
            .hard {{ background: #FFEBEE; border-left: 4px solid #F44336; }}
            .medium {{ background: #FFF3E0; border-left: 4px solid #FF9800; }}
            .simple {{ background: #E8F5E9; border-left: 4px solid #4CAF50; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üöÄ Xiaozhi MCP v3.6</h1>
            <p>SMART Tiered Gemini Selection</p>
        </div>
        
        <h2>üéØ Your Model Tiers:</h2>
        
        <div class="tier hard">
            <h3>üî¥ HARD Tasks ‚Üí Pro Tier</h3>
            <p><strong>Models:</strong> gemini-2.5-pro ‚Üí gemini-2.5-flash ‚Üí gemini-2.5-flash-lite</p>
            <p><strong>For:</strong> Complex analysis, coding, detailed explanations</p>
        </div>
        
        <div class="tier medium">
            <h3>üü° MEDIUM Tasks ‚Üí Flash Tier</h3>
            <p><strong>Models:</strong> gemini-2.5-flash ‚Üí gemini-2.5-flash-lite ‚Üí gemini-2.0-flash</p>
            <p><strong>For:</strong> General Q&A, explanations, guides</p>
        </div>
        
        <div class="tier simple">
            <h3>üü¢ SIMPLE Tasks ‚Üí Flash-Lite Tier</h3>
            <p><strong>Models:</strong> gemini-2.5-flash-lite</p>
            <p><strong>For:</strong> Quick answers, facts, simple questions</p>
        </div>
        
        <p>Uptime: {hours}h {minutes}m {seconds}s | Cache: {len(gemini_cache)} items</p>
    </body>
    </html>
    """

@app.route('/health')
def health_check():
    return jsonify({
        "status": "healthy",
        "version": "3.6.0",
        "tiers": SmartModelSelector.TIERS,
        "cache_size": len(gemini_cache)
    }), 200

@app.route('/test-smart/<query>')
def test_smart(query):
    """Test the smart tier selection."""
    result = ask_gemini_smart(query)
    return jsonify({
        "query": query,
        "result": result,
        "cache_size": len(gemini_cache)
    }), 200

def run_web_server():
    app.run(host='0.0.0.0', port=3000, debug=False, threaded=True)

# ================= MAIN =================
async def mcp_bridge():
    """WebSocket bridge."""
    reconnect_delay = 2
    while True:
        try:
            async with websockets.connect(
                XIAOZHI_WS,
                ping_interval=25,
                ping_timeout=15,
                close_timeout=10
            ) as websocket:
                logger.info("‚úÖ Connected to Xiaozhi")
                reconnect_delay = 2
                
                async for raw_message in websocket:
                    try:
                        data = json.loads(raw_message)
                        message_id = data.get("id")
                        method = data.get("method", "")
                        params = data.get("params", {})
                        
                        response = None
                        
                        if method == "ping":
                            response = MCPProtocolHandler.handle_ping(message_id)
                        elif method == "initialize":
                            response = MCPProtocolHandler.handle_initialize(message_id)
                            logger.info("‚úÖ Initialized")
                        elif method == "tools/list":
                            response = MCPProtocolHandler.handle_tools_list(message_id)
                            logger.info("‚úÖ Sent tools list")
                        elif method == "tools/call":
                            response = MCPProtocolHandler.handle_tools_call(message_id, params)
                            tool_name = params.get("name", "")
                            logger.info(f"‚úÖ Processed {tool_name}")
                        elif method == "notifications/initialized":
                            continue
                        else:
                            response = {"jsonrpc": "2.0", "id": message_id, "error": {"code": -32601, "message": f"Unknown: {method}"}}
                        
                        if response:
                            await websocket.send(json.dumps(response))
                            
                    except Exception as e:
                        logger.error(f"Message error: {e}")
        
        except Exception as e:
            logger.error(f"Connection error: {e}")
            logger.info(f"‚è≥ Reconnecting in {reconnect_delay}s...")
            await asyncio.sleep(reconnect_delay)
            reconnect_delay = min(reconnect_delay * 1.5, 60)

async def main():
    logger.info("üöÄ Starting Xiaozhi MCP v3.6 - Smart Tiered Gemini")
    logger.info(f"üìä Gemini: {'‚úÖ Configured' if GEMINI_API_KEY else '‚ùå Not configured'}")
    
    # Start web server
    web_thread = threading.Thread(target=run_web_server, daemon=True)
    web_thread.start()
    logger.info("üåê Web server on http://0.0.0.0:3000")
    
    # Start MCP bridge
    await mcp_bridge()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üëã Stopped")
    except Exception as e:
        logger.error(f"Fatal: {e}")